<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>UfoRoblog</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2018-05-24T23:00:08+01:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Ascanio Vimercati Sanseverino</name>
   <email>ascanio.vimercati@gmail.com</email>
 </author>

 
 <entry>
   <title>The .flac format &#58; linear predictive coding and rice codes</title>
   <link href="http://localhost:4000/2018/04/01/flac-format/"/>
   <updated>2018-04-01T00:00:00+01:00</updated>
   <id>http://localhost:4000/2018/04/01/flac-format</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;FLAC&lt;/strong&gt; stands for &lt;strong&gt;F&lt;/strong&gt;ree &lt;strong&gt;L&lt;/strong&gt;ossless &lt;strong&gt;A&lt;/strong&gt;udio &lt;strong&gt;C&lt;/strong&gt;odec. It’s the standard open source codec used nowadays for lossless audio compression. In this post I would like to take the .flac format as an example and focus on two core step of the process of lossless audio compression: linear predictive coding and entropy coding with Rice codes.&lt;/p&gt;

&lt;h3 id=&quot;tltr&quot;&gt;TL;TR&lt;/h3&gt;
&lt;p&gt;Most lossless audio compression codecs work very similarly: first the audio stream is split into blocks, and then each one is compressed. Compression is achieved identifying and modelling structure in the signal: any explicit structure and repeating pattern is redundant by definition and can be instead represented more efficiently using a mathematical model and its parameters to approximate it. Usually a very simple model is used, from constant values to model silence to the more flexible linear predictive coding (a linear autoregressive model). In order to recreate the signal exactly, the approximation residuals are also saved but using a coding scheme optimised for their distribution. Linear predictive coding leads to residual Laplace-distributed and hence usually the Rice coding scheme is used.&lt;/p&gt;

&lt;h1 id=&quot;linear-predictive-coding&quot;&gt;Linear predictive coding&lt;/h1&gt;
&lt;p&gt;Linear predictive coding is a modelling technique equivalent to an \(AR(p)\) auto regressive model: the signal \(X_t\) at time \(t\) is modelled as a linear combination of its previous \(p\) values.&lt;/p&gt;

&lt;p&gt;More formally, let \({X_t}\) be a wide sense stationary (WSS) stochastic process with zero mean and with the \(AR(p)\) property, which is:&lt;/p&gt;

&lt;p&gt;\[
\begin{equation}\label{eqn:wss_1}
E[X_t] = \mu = 0   \quad \forall t
\end{equation}
\]&lt;/p&gt;

&lt;p&gt;\[
\begin{equation}\label{eqn:wss_2}
Cov[X_{t}, X_{t + \tau}] = Cov[X_{t+h}, X_{t +h + \tau}]  \quad \forall t \ \forall h &lt;br /&gt;
\end{equation}
\]&lt;/p&gt;

&lt;p&gt;From \ref{eqn:wss_1} and \ref{eqn:wss_2} it follows that the covariance function only depends on the lag \(\tau\) and can be expressed in terms of the autocorrelation function \(R(\tau)\):&lt;/p&gt;

&lt;p&gt;\[\begin{equation}\label{eqn:autocorr}
Cov[X_{t}, X_{t + \tau}] = E[(X_t - \mu)(X_{t + \tau} - \mu)] = E[X_tX_{t+\tau}] = R(\tau)
\end{equation}\]
where \(R(\tau) = R(-\tau)\) is symmetric.&lt;/p&gt;

&lt;p&gt;Assume that we wanted to approximate each value \(X_t\) as a linear combination of its previous \(p\) values:&lt;/p&gt;

&lt;p&gt;\[ \hat{X_t} = \sum_{k = 1}^{p}\alpha_k X_{t-k} \]&lt;/p&gt;

&lt;p&gt;now let \(\boldsymbol{\boldsymbol{\alpha}} =  \begin{bmatrix} \alpha_1 &amp;amp; \cdots &amp;amp; \alpha_p \end{bmatrix}^\top \), the approximation error \(\epsilon_t\) at time t is the random variable:&lt;/p&gt;

&lt;p&gt;\[
\epsilon_t    =  X_t - \hat{X_t} 
            =  X_t - \mathbf{\boldsymbol{\alpha}}^\top \begin{bmatrix} X_{t-1} \\ \vdots \\ X_{t-p} \end{bmatrix} 
            =  X_t - \mathbf{\boldsymbol{\alpha}}^\top \mathbf{X_{t-1:t-p}}
\]&lt;/p&gt;

&lt;p&gt;We can then chose \(\boldsymbol{\alpha}\) so that it minimises the expected squared estimation error.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Lemma: Orthogonality principle.&lt;/strong&gt;&lt;br /&gt;
The estimate \(\hat{X_t} \) minimises the expected squared error if and only if the estimation error \(\epsilon_t\) is orthogonal to the \(p\) random variables \(X_{t-1}, …, X_{t-p}\), ie if and only if:
\[
E[\epsilon_t X_{t-i}] = 0 \quad \forall 1 \leq i \leq p
\]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We can use the orthogonality principle to derive the Yule-Walker equations:
\[
\begin{equation}\label{eqn:yw}
\begin{split}
\mathbf{0} = &amp;amp; E[\epsilon_t \mathbf{X_{t-1:t-p}}] \\
           = &amp;amp; E[(X_t - \mathbf{X_{t-1:t-p}}^\top\alpha)\mathbf{X_{t-1:t-p}}] \\
           = &amp;amp; E[X_t \mathbf{X_{t-1:t-p}} - \mathbf{X_{t-1:t-p}}^\top\boldsymbol{\alpha}\mathbf{X_{t-1:t-p}}] \\
            = &amp;amp; E[X_t \mathbf{X_{t-1:t-p}} - trace(\mathbf{X_{t-1:t-p}}^\top\boldsymbol{\alpha}\mathbf{X_{t-1:t-p}})] \\
            = &amp;amp; E[X_t \mathbf{X_{t-1:t-p}} - trace(\mathbf{X_{t-1:t-p}}\mathbf{X_{t-1:t-p}}^\top\boldsymbol{\alpha})] \\
            = &amp;amp; E[X_t \mathbf{X_{t-1:t-p}} - \mathbf{X_{t-1:t-p}}\mathbf{X_{t-1:t-p}}^\top\boldsymbol{\alpha}] \\
            = &amp;amp; E[X_t \mathbf{X_{t-1:t-p}}] - E[ \mathbf{X_{t-1:t-p}}\mathbf{X_{t-1:t-p}}^\top]\boldsymbol{\alpha} \\
\end{split}
\end{equation}
\]&lt;/p&gt;

&lt;p&gt;Using \ref{eqn:autocorr} we can rewrite \( E[ \mathbf{X_{t-1:t-p}}\mathbf{X_{t-1:t-p}}]^\top = \mathbf{R}\)  as:&lt;/p&gt;

&lt;p&gt;\[
\mathbf{R} = 
\begin{bmatrix}
R(0) &amp;amp; R(1) &amp;amp; R(2) &amp;amp; \cdots &amp;amp; R(p-1) \\
R(1) &amp;amp; R(0) &amp;amp; R(1) &amp;amp; \cdots &amp;amp; R(p-2) \\
R(2) &amp;amp; R(1) &amp;amp; R(0) &amp;amp; \cdots &amp;amp; R(p-3) \\
\vdots  &amp;amp; \vdots   &amp;amp; \vdots &amp;amp;  &amp;amp; \vdots \\
R(p-1) &amp;amp; R(p-2) &amp;amp; R(p-3) &amp;amp; \cdots &amp;amp; R(0) \\
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;and by letting \(\mathbf{r} = \begin{bmatrix} R(1) &amp;amp; \cdots &amp;amp; R(p) \end{bmatrix}^\top\) we can rewrite \ref{eqn:yw} as:&lt;/p&gt;

&lt;p&gt;\begin{equation}\label{eqn:yh2}
\mathbf{R}\boldsymbol{\alpha} = \mathbf{r} &lt;br /&gt;
\end{equation}&lt;/p&gt;

&lt;p&gt;which can be solved for \(\boldsymbol{\alpha}\) given estimates of \(\mathbf{R}\) and \(\mathbf{r}\)&lt;/p&gt;

&lt;p&gt;Note that the matrix \(\mathbf{R}\) is a &lt;em&gt;toeplitz&lt;/em&gt; matrix. A toeplitz matrix has the property that each descending diagonal from left to right is constant and there exist an algorithm, the Levinson–Durbin recursion, to solve the system in \(O(p^2)\) instead of simply inverting \(\mathbf{R}\), which would have a cost \(O(p^3)\). The overall computational cost is thus \(O(np + p^2)\) where the first term comes from the cost of computing the sample estimate of the auto correlation matrix.&lt;/p&gt;

&lt;h2 id=&quot;comparison-with-ordinary-least-squares&quot;&gt;Comparison with ordinary least squares&lt;/h2&gt;
&lt;p&gt;The previous model could have been expressed as an instance of a general linear regression model whose covariates are the \(p\) lagged values. The solution minimises for the expected mean squared error directly, without making assumptions about the underlying WSS property. This leads to a very similar but more expensive solution.&lt;/p&gt;

&lt;p&gt;The normal equations for the general linear regression problem are:
\[
(\mathbf{X}^\top \mathbf{X})\mathbf{\boldsymbol{\alpha}} =  \mathbf{X}^\top \mathbf{y}
\]&lt;/p&gt;

&lt;p&gt;where:
\[
\mathbf{y} = \begin{bmatrix}
    y_{p+1}\\
    \vdots \\
    y_n \\
    \vdots \\
    y_N
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;\[
\mathbf{X} = \begin{bmatrix}
y_p  &amp;amp; y_{p-1} &amp;amp; \cdots &amp;amp; y_0 \\
\vdots  &amp;amp;\vdots   &amp;amp;  &amp;amp;\vdots   \\
y_n  &amp;amp; y_{n-1} &amp;amp; \cdots &amp;amp; y_{n-p} \\
\vdots  &amp;amp;\vdots   &amp;amp;  &amp;amp;\vdots   \\
y_N  &amp;amp; y_{N-1} &amp;amp; \cdots &amp;amp; y_{N-p} \\ &lt;br /&gt;
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;\[
\mathbf{\boldsymbol{\alpha}} = \begin{bmatrix}
    \alpha_1\\
    \vdots \\
    \alpha_p \\
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;It is easy to notice the similarities with equation \ref{eqn:yh2} : this solution is computing the sample estimates of \(\mathbf{R}\) and \( \mathbf{r}\), but using a different subset of samples for each estimate. Because of this the estimates have to be recomputed for each entry, and the toeplitz structure is lost, rsulting in higher estimation costs to build the matrices and higher matrix inversion costs to solve for \(\boldsymbol{\alpha}\). The computational complexity of this solution is \(O(n^2p + p^3)\), where the first term comes from the inner product of the matrix of covariates and the second term from its inversion. Usually \(n \gg p\) and thus this solution has complexity \(O(n^2p)\) which is much worst than the previous result of \(O(np)\) when using \ref{eqn:yh2}&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Entropy coding and Rice codes&lt;/em&gt;
Suppose one was to observe a realisation of the above mentioned stochastic process and used LPC (or some other model) to approximate it. In order to reconstruct the original signal without approximation error it is necessary to know what the exact realisation of the reconstruction error were and add them to the reconstruction. The problem of efficiently storing these residuals is an instance of &lt;em&gt;entropy encoding&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Assume the observed data can be modelled as a discrete random variable (note that we redefine (X) from the previous section):
\[ X : \Omega \rightarrow \boldsymbol{\chi} \]
where \(P(x \in X) = P(x)\)&lt;/p&gt;

&lt;p&gt;now, given an alphate of symbols \(\Sigma\), in the digital case \(\Sigma = {0, 1}\), a code is defined as
\[C : \boldsymbol{\chi} \rightarrow \Sigma^*\]
\(C(x)\) is the code associated with \(x\). Let its length be \(l(C(x))\), then the expected length of a code is:
\[l(C) = E_{x \sim P(x)}[l(C(x))] = \sum_{x \in \chi} P(x)l(C(x))\]&lt;/p&gt;

&lt;p&gt;when designing or choosing a code the objective is to minimise \(l(C)\) for the distribution of input words \(p(x)\), so that the encoded input will require on average the least number of symbols. This is the problem referred to as &lt;em&gt;entropy coding&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A good general code that works for any input distribution is the Huffman code [1], and is for example used in the popular compression tool &lt;em&gt;gzip&lt;/em&gt;. When the distribution of the input is known a priori though it is possible to use a coding scheme tailored for that distribution.&lt;/p&gt;

&lt;p&gt;For the case of alphabets following a geometric distribution the optimal prefix code (a prefix code is a code where no code-word is allowed to be the prefix of another code-word) is the Golomb code [3], making it highly suitable for situations in which the occurrence of small values in the input stream is significantly more likely than large values, as for the case of the reconstruction errors.&lt;/p&gt;

&lt;p&gt;Golomb coding [2] was invented by Solomon W. Golomb in the 1960s. It takes the input \(s\) and divides it by the tunable parameter \(m\). First the quotient is encoded in &lt;em&gt;unary coding&lt;/em&gt; and then the remainder is econded in \textit{truncated binary coding}. Rice coding is a special case of Golomb coding where (m = 2^k), which is faster to encode and decode thanks to the usage of powers of 2. It is very intuitive to understand how the coding scheme works just by looking at the example in the following table&lt;/p&gt;

&lt;p&gt;#Entropy coding and Rice codes
Suppose one was to observe a realisation of the above mentioned stochastic process and used LPC (or some other model) to approximate it. In order to reconstruct the original signal without approximation error it is necessary to know what the exact realisation of the reconstruction error were and add them to the reconstruction. The problem of efficiently storing these residuals is an instance of &lt;em&gt;entropy encoding&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Assume the observed data can be modelled as a discrete random variable (note that we redefine \(X\) from the previous section):
\[ X : \Omega \rightarrow \boldsymbol{\chi} \]
where \(P(x \in X) = P(x)\)&lt;/p&gt;

&lt;p&gt;now, given an alphate of symbols \(\Sigma\), in the digital case \(\Sigma = {0, 1}\), a code is defined as
[\C : \boldsymbol{\chi} \rightarrow \Sigma^*\]
\(C(x)\) is the code associated with \(x\). Let its length be \(l(C(x))\), then the expected length of a code is:
\[l(C) = E_{x \sim P(x)}[l(C(x))] = \sum_{x \in \chi} P(x)l(C(x))\]&lt;/p&gt;

&lt;p&gt;when designing or choosing a code the objective is to minimise \(l(C)\) for the distribution of input words \(p(x)\), so that the encoded input will require on average the least number of symbols. This is the problem referred to as &lt;em&gt;entropy coding&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A good general code that works for any input distribution is the Huffman code [1], and is for example used in the popular compression tool &lt;em&gt;gzip&lt;/em&gt;. When the distribution of the input is known a priori though it is possible to use a coding scheme tailored for that distribution.&lt;/p&gt;

&lt;p&gt;For the case of alphabets following a geometric distribution the optimal prefix code (a prefix code is a code where no code-word is allowed to be the prefix of another code-word) is the Golomb code \cite{gallager1975optimal}, making it highly suitable for situations in which the occurrence of small values in the input stream is significantly more likely than large values, as for the case of the reconstruction errors.&lt;/p&gt;

&lt;p&gt;Golomb coding \cite{golomb1966run} was invented by Solomon W. Golomb in the 1960s. It takes the input \(s\) and divides it by the tunable parameter \(m\). First the quotient is encoded in &lt;em&gt;unary coding&lt;/em&gt; and then the remainder is econded in &lt;em&gt;truncated binary coding&lt;/em&gt;. Rice coding is a special case of Golomb coding where \(m = 2^k\), which is faster to encode and decode thanks to the usage of powers of 2. It is very intuitive to understand how the coding scheme works just by looking at the follwowing table example&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Golomb&lt;/th&gt;
      &lt;th&gt;m=1&lt;/th&gt;
      &lt;th&gt;m=2&lt;/th&gt;
      &lt;th&gt;m=3&lt;/th&gt;
      &lt;th&gt;m=4&lt;/th&gt;
      &lt;th&gt;m=5&lt;/th&gt;
      &lt;th&gt;m=6&lt;/th&gt;
      &lt;th&gt;m=7&lt;/th&gt;
      &lt;th&gt;m=8&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;strong&gt;Rice&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;k=0&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;k=1&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;k=2&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;k=3&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;s = 0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;00&lt;/td&gt;
      &lt;td&gt;00&lt;/td&gt;
      &lt;td&gt;000&lt;/td&gt;
      &lt;td&gt;000&lt;/td&gt;
      &lt;td&gt;000&lt;/td&gt;
      &lt;td&gt;000&lt;/td&gt;
      &lt;td&gt;0000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;01&lt;/td&gt;
      &lt;td&gt;010&lt;/td&gt;
      &lt;td&gt;001&lt;/td&gt;
      &lt;td&gt;001&lt;/td&gt;
      &lt;td&gt;001&lt;/td&gt;
      &lt;td&gt;0010&lt;/td&gt;
      &lt;td&gt;0001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;011&lt;/td&gt;
      &lt;td&gt;010&lt;/td&gt;
      &lt;td&gt;010&lt;/td&gt;
      &lt;td&gt;0100&lt;/td&gt;
      &lt;td&gt;0011&lt;/td&gt;
      &lt;td&gt;0010&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td&gt;1110&lt;/td&gt;
      &lt;td&gt;101&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;011&lt;/td&gt;
      &lt;td&gt;0110&lt;/td&gt;
      &lt;td&gt;0101&lt;/td&gt;
      &lt;td&gt;0100&lt;/td&gt;
      &lt;td&gt;0011&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td&gt;11110&lt;/td&gt;
      &lt;td&gt;1100&lt;/td&gt;
      &lt;td&gt;1010&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;0111&lt;/td&gt;
      &lt;td&gt;0110&lt;/td&gt;
      &lt;td&gt;0101&lt;/td&gt;
      &lt;td&gt;0100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5&lt;/td&gt;
      &lt;td&gt;\(1^50\)&lt;/td&gt;
      &lt;td&gt;1101&lt;/td&gt;
      &lt;td&gt;1011&lt;/td&gt;
      &lt;td&gt;1001&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;0111&lt;/td&gt;
      &lt;td&gt;0110&lt;/td&gt;
      &lt;td&gt;0101&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6&lt;/td&gt;
      &lt;td&gt;\(1^60\)&lt;/td&gt;
      &lt;td&gt;11100&lt;/td&gt;
      &lt;td&gt;1100&lt;/td&gt;
      &lt;td&gt;11010&lt;/td&gt;
      &lt;td&gt;1001&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;0111&lt;/td&gt;
      &lt;td&gt;0110&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7&lt;/td&gt;
      &lt;td&gt;\(1^70\)&lt;/td&gt;
      &lt;td&gt;11101&lt;/td&gt;
      &lt;td&gt;11010&lt;/td&gt;
      &lt;td&gt;1011&lt;/td&gt;
      &lt;td&gt;1010&lt;/td&gt;
      &lt;td&gt;1001&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;0111&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8&lt;/td&gt;
      &lt;td&gt;\(1^80\)&lt;/td&gt;
      &lt;td&gt;111100&lt;/td&gt;
      &lt;td&gt;11100&lt;/td&gt;
      &lt;td&gt;11000&lt;/td&gt;
      &lt;td&gt;10110&lt;/td&gt;
      &lt;td&gt;10100&lt;/td&gt;
      &lt;td&gt;10010&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;the-flac-codec&quot;&gt;The FLAC codec&lt;/h1&gt;
&lt;p&gt;At this point it should be intuitive how modelling and entropy coding can be combined to form a lossless audio codec. FLAC is mainly based on &lt;em&gt;shorten&lt;/em&gt; [4], with additional features for more convenient use in real case consumer scenarios.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/flac_stream.png&quot; alt=&quot;The FLAC stream&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The basic structure of a FLAC stream is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the four byte string “fLaC” to identify the stream&lt;/li&gt;
  &lt;li&gt;the STREAMINFO metadata block&lt;/li&gt;
  &lt;li&gt;zero or more other metadata blocks&lt;/li&gt;
  &lt;li&gt;one of more audio frames&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;FLAC defines various metadata blocks. They can be used for padding, seek tables, tags, cue sheets, and even application-specific data. There is no metadata block for the ID3 tags (where the artist, album, etc info are usually stored) but the world doesn’t really care and most decoders, including the reference one, know how to handle them anyway.&lt;br /&gt;
The only mandatory block is the STREAMINFO block. This block contains information like the sample rate, number of channels, etc., and data that can help the decoder manage its buffers, like the minimum and maximum data rate and minimum and maximum block size. Also included in the STREAMINFO block is the MD5 signature of the unencoded audio data, useful for checking an entire stream for transmission errors.&lt;/p&gt;

&lt;p&gt;Following the metadata blocks there is the sequence of frames containing the compressed audio stream: FLAC first splits the un-encoded audio data into blocks and then encodes each block separately. The blocking procedure serves two purposes: it is possible to un-compress, edit and re-compress only a subset of the frames at a time and it allows for the compression parameters to change over time, which is ideal since the audio signal is most certainly not stationary (but can be approximated as such within each block). The encoded data block is then packed into a frame with a header and a footer, and is appended to the stream. There is a trade-off in choosing the block size: smaller blocks allow for better compression but require more frame headers / footers to be stored. The reference implementation defaults to a block-size of 4096 samples, which at a sampling rate of 44.1kHz equals about 93 milliseconds but in theory blocks could be of variable length.&lt;/p&gt;

&lt;h2 id=&quot;compressing-the-audio-signal-in-each-data-block&quot;&gt;Compressing the audio signal in each data block&lt;/h2&gt;
&lt;p&gt;The raw encoding of an audio signal is extremely space inefficient: your common 16 bit 44.1kHz raw audio signal is encoded by storing each second of audio signal as a sequence of 44100 16 bit numbers (that is 88 KB/s!) representing the quantised discrete values of the audio wave over time. Any structure in the audio signal is just be encoded as it comes: any moment of silence takes as much space as the most explosive of the cymbals!&lt;/p&gt;

&lt;p&gt;At a high level, the compression procedure reduces the redundancy in the raw representation by identifying and modelling structured patterns in the raw signal: it exploits this patterns to approximate the raw signal using a mathematical function, and then also stores the approximation error so that it can revert it and recreate the original signal with no loss of information. Compression is achieved because storing both parts is much more efficient than storing the original raw signal: the approximated signal is saved by simply storing its mathematical model while the approximation error is saved more efficiently because of the use of Rice coding. Errors are almost always small and storing small numbers takes less space. In addition to this, their distribution can be assumed a priori, which allows to use a coding scheme optimised for it.&lt;/p&gt;

&lt;p&gt;More specifically, the model being fitted to the signal by FLAC can either be a constant value (for silent moments), LPC or a fixed polynomial predictor from a subset of 4 that usually work well. Fixed polynomial prediction is much faster, but less accurate than LPC. The higher the maximum LPC order \(p\), the slower but more accurate the model will be. However, there are diminishing returns with increasing orders: \(p = 1\) already leads to good compression, while higher orders increase the required computation time with smaller compression benefits and in addition to this the LPC parameters take more space to save. The approximation error is then obtained by subtracting the original signal with approximated signal, and since they are (empirically) Laplace distributed they can efficiently be encoded using a simple symmetric variation of the Rice coding scheme.
These procedure is applied to both channel, left and right independently. A neat trick that can be used to often increase the compression of the block is to switch from left-right to mid-side (mid = (left + right) / 2, side = left - right).&lt;/p&gt;

&lt;p&gt;According to the modelling choice, the block is then encoded. First a sub frame is built, marking the model used in the header and encoding the audio content in the sub frame body. For example, in the case of LPC the sub frame body contains the (p) initial samples, the LPC coefficient and then the sequence of the encoded residuals. Finally the frame is constructed by preceding the sub frame with a frame header ant trailing it with a frame footer. The header starts with a sync code, and contains the minimum information necessary for a decoder to play the stream, like sample rate, bits per sample, etc. It also contains the block or sample number and an 8-bit CRC of the frame header. The frame footer contains a 16-bit CRC of the entire encoded frame for error detection. If the reference decoder detects a CRC error it will generate a silent block.&lt;/p&gt;

&lt;h1 id=&quot;bibliography&quot;&gt;Bibliography&lt;/h1&gt;
&lt;p&gt;[1] David A Huffman. “A method for the construction of minimum-redundancycodes”. In: &lt;em&gt;Proceedings of the IRE&lt;/em&gt; 40.9 (1952), pp. 1098–1101.&lt;/p&gt;

&lt;p&gt;[2] Solomon Golomb. “Run-length encodings (Corresp.)” In: &lt;em&gt;IEEE trans-actions on information theory&lt;/em&gt; 12.3 (1966), pp. 399–401.&lt;/p&gt;

&lt;p&gt;[3] Robert Gallager and David Van Voorhis. “Optimal source codes for ge-ometrically distributed integer alphabets (corresp.)” In: &lt;em&gt;IEEE Trans-actions on Information theory&lt;/em&gt; 21.2 (1975), pp. 228–230.&lt;/p&gt;

&lt;p&gt;[4] Tony Robinson. SHORTEN: Simple lossless and near-lossless wave-form compression. 1994.&lt;/p&gt;

</content>
 </entry>
 

</feed>
